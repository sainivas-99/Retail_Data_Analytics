---
title: "Retail Data Analytics"
output: html_document
date: "2023-10-25"
---
```{r}
features_data <- read.csv("./Datasets/Features data set.csv",na.strings = "NA")
sales_data <- read.csv("./Datasets/sales data-set.csv")
stores_data  <- read.csv("./Datasets/stores data-set.csv")
```
```{r}
str(stores_data)
unique(stores_data$Type)
dim(stores_data)
```
```{r}
summary(stores_data)
```
#Data Preprocessing:
## Problems with Sales data:
  1. Type: Factor...?
```{r}
str(sales_data)
dim(sales_data)
```
```{r}
summary(sales_data)
```
##Problems with Sales data:
  1. Date : chr-> date : Done
  2. Weekly sales: why is the Weekly_Sales min a negative value?
  
##Solving for 1 in sales
```{r}
sales_data$Date <- as.Date(sales_data$Date, format = "%d/%m/%Y")
str(sales_data)
```
##Looking into 2 in sales:
```{r}
#View(sales_data)
```
The Weekly_sales have many negative values, so we can assume that this is due
to maybe returns or refunds to customers, theft or shrinkage or losses or maybe
due to promotional discounts. So we keep them as it is.
```{r}
head(features_data)
```

```{r}
str(features_data)
dim(features_data)
```
```{r}
summary(features_data)
```

## Problems with Features Data:
  1. Date column: chr -> date
  2. Markdown columns: have NA values, how to deal with them?
    a. Probably put it as zero, as the promotion is not available all time
       in all stores.
  3. CPI has NA values.
  4. Unemployment Rate has NA values, how to deal with them?

##Solving 1 for features: 
```{r}
features_data$Date <- as.Date(features_data$Date, format = "%d/%m/%Y")
summary(features_data$Date)
```
##Looking into 2 for features:
```{r}
MD1.na_count <- sum(is.na(features_data$MarkDown1))
MD1.na_count
```
```{r}
MD2.na_count <- sum(is.na(features_data$MarkDown2))
MD2.na_count
```
```{r}
MD3.na_count <- sum(is.na(features_data$MarkDown3))
MD3.na_count
```
```{r}
MD4.na_count <- sum(is.na(features_data$MarkDown4))
MD4.na_count
```
```{r}
MD5.na_count <- sum(is.na(features_data$MarkDown5))
MD5.na_count
```
```{r}
features_data$MarkDown1 <- ifelse(is.na(features_data$MarkDown1),0,
                                  features_data$MarkDown1)
features_data$MarkDown2 <- ifelse(is.na(features_data$MarkDown2),0,
                                  features_data$MarkDown2)
features_data$MarkDown3 <- ifelse(is.na(features_data$MarkDown3),0,
                                  features_data$MarkDown3)
features_data$MarkDown4 <- ifelse(is.na(features_data$MarkDown4),0,
                                  features_data$MarkDown4)
features_data$MarkDown5 <- ifelse(is.na(features_data$MarkDown5),0,
                                  features_data$MarkDown5)
```
```{r}
MD1.na_count <- sum(is.na(features_data$MarkDown1))
MD1.na_count
```
```{r}
MD2.na_count <- sum(is.na(features_data$MarkDown2))
MD2.na_count
```
```{r}
MD3.na_count <- sum(is.na(features_data$MarkDown3))
MD3.na_count
```
```{r}
MD4.na_count <- sum(is.na(features_data$MarkDown4))
MD4.na_count
```
```{r}
MD5.na_count <- sum(is.na(features_data$MarkDown5))
MD5.na_count
```

## Dealing with 3 in features
```{r}
CPI.na.count <- sum(is.na(features_data$CPI))
CPI.na.count
```
```{r}
Unemp.na.count <- sum(is.na(features_data$Unemployment))
Unemp.na.count
```
```{r}
library(dplyr)
cpi_features_data <- filter(features_data, !is.na(CPI)& !is.na(Unemployment))
head(cpi_features_data)
```
```{r}
features_data <- na.omit(features_data)
dim(features_data)
```


```{r}
str(stores_data)
```
```{r}
str(sales_data)
```

```{r}
head(stores_data)
```
```{r}
head(features_data)
```
```{r}
names(features_data)
names(sales_data)
```
```{r}
library(lubridate)
sales_data$Week <- week(sales_data$Date)
sales_data$Month <- month(sales_data$Date)
sales_data$Year <- year(sales_data$Date)
```
```{r}
head(sales_data)
```
```{r}
features_data$Week <- week(features_data$Date)
features_data$Month <- month(features_data$Date)
features_data$Year <- year(features_data$Date)
```
```{r}
head(features_data)
```
```{r}
summary(sales_data)
```
```{r}
filtered_sales <- sales_data %>% filter(Year %in% c(2010, 2011, 2012))
head(filtered_sales)
```


```{r}
weekly_sales_sum <- filtered_sales %>% group_by(Year, Week) %>%
  summarise(Total_Weekly_Sales = sum(Weekly_Sales))
head(weekly_sales_sum)
```


```{r}
library(ggplot2)
ggplot(weekly_sales_sum, aes(x = Week, y = Total_Weekly_Sales, color = factor(Year))) +
  geom_line() +
  labs(
    title = "Total Weekly Sales for 2010, 2011, and 2012"
  ) +
  scale_x_continuous(breaks = seq(0, 52, by = 4)) +
  scale_y_continuous(labels = scales::number_format(scale = 1e-3, suffix = "K"))+
  scale_color_discrete(name = "Year")
```
Weeks 6, 14, 26, 47, 51 have local maxima in the total sum over the years 2010, 2011
and 2012. The weeks 6, 47 and 51 are holiday weeks and hence have higher sales,
indicating a positive impact of holidays on sales.
```{r}
mean_monthly_sales <- filtered_sales %>% group_by(Year, Month) %>%
  summarise(Mean_Monthly_Sales = mean(Weekly_Sales))
head(mean_monthly_sales)
```
```{r}
ggplot(mean_monthly_sales, mapping = aes(x = Month, y = Mean_Monthly_Sales,
                                         color = factor(Year)))+
  geom_line()+
  labs(title = "Mean of Sales over weeks")+
  scale_x_continuous(breaks = seq(0, 12, by = 1))+
  scale_color_discrete(name = "Year")
```
The sales peaked at months 2, 6, 8 and are increasing in months 10-12, as these
have holidays. Moreover, the mean sales are comparably greater during months:
5 to 7 excluding holiday effect during the months 10-12. This is likely
indicating that the sales are higher during the summer.

```{r}
features_sales_merged <- merge(sales_data, features_data,
                               by = c("Store","Date","IsHoliday","Week",
                                      "Month","Year"))
merged_data <- merge(stores_data,features_sales_merged,
               by = "Store")
head(merged_data)
```


Filling NA valued fields to 0
```{r}
# Fill NA/missing values with 0
merged_data[is.na(merged_data)] <- 0

# Print the first few rows of the dataframe
head(merged_data)

# Display information about the dataframe
str(merged_data)

# Display summary statistics of the dataframe
summary(merged_data)

# Sort the dataframe by 'Date'
merged_data <- merged_data[order(merged_data$Date), ]
```


```{r}
# Extract the 'Dept' column from the dataframe df
dept_list <- merged_data$Dept

# Convert to a list and get unique values
dept_list <- unique(dept_list)

# Convert the elements to strings
depts <- as.character(dept_list)

# Create a copy of depts
depts_res <- depts
```

To study how different categories of markdowns affect sales, a single function, resch_markdown, was compiled, which allows you to immediately display all the graphs and tables necessary for analysis. The components of the reserch_markdown(markdown_category) function are presented below.

Function for determining the average value for each of the departments:
```{r}
filter_dept_sales <- function(merged_data) {
  if (nrow(merged_data) == 0) {
    return("There are no records in the data frame")
  }
  
  merged_dept_val <- numeric(length(dept_list))
  
  for (i in seq_along(dept_list)) {
    merged_dept_val_add <- merged_data[merged_data$Dept == dept_list[i], ]
    val <- merged_dept_val_add$Weekly_Sales
    value <- mean(val, na.rm = TRUE)
    merged_dept_val[i] <- value
  }
  
  return(merged_dept_val)
}

```

Below is the function for plotting a graph sorted in descending order:
```{r}
paint_graph <- function(merged_dept, depts) {
  merged_dept_val <- merged_dept
  depts <- depts

  # Sorting
  sorted_indices <- order(merged_dept_val, decreasing = TRUE)
  merged_dept_val <- merged_dept_val[sorted_indices]
  depts <- depts[sorted_indices]

  # Function to add labels
  autolabel <- function(rects, labels = NULL, height_factor = 1.5) {
    for (i in seq_along(rects)) {
      height <- rects[i]
      label <- ifelse(!is.null(labels), labels[i], as.character(height))
      # text(x = i, y = height * 0.98, label = label, pos = 3, offset = 0.05, cex = 0.7, col = "black")
    }
  }

  # Plotting
  barplot(merged_dept_val, names.arg = depts, col = "steelblue", main = "Sales by Department", xlab = "Sales", ylab = "Departments")
  autolabel(merged_dept_val, labels = depts, height_factor = 0.5)
}

```


Below is the function for displaying a table that allows you to study the impact of markdowns of various categories on sales in departments:
```{r}
show_table_depts <- function(depts, val_no, val_have, dif, dif_part, mark) {
  table_mark_list <- data.frame('Department' = depts, "No markdown" = val_no, "There is a markdown mark category" = val_have, "With markdown - without markdown" = dif, "Percentage change" = dif_part)
  table_mark <- table_mark_list[order(table_mark_list$`Percentage change`, decreasing = TRUE), ]
  colnames(table_mark) <- c('Department', 'No markdown', paste('There is a markdown', mark, 'categories'), 'With markdown - without markdown', 'Percentage change')
  
  print(table_mark)
}

```


Below is the function for constructing a double bar graph:
```{r}
paint_double_graph <- function(depts_res, y1, y2) {
  x1 <- seq(1, length(depts_res)) - 0.15
  x2 <- seq(1, length(depts_res)) + 0.25
  x3 <- seq(1, length(depts_res))
  y3 <- rep(0, length(depts_res))

  # Creating the plot
  par(mfrow = c(1, 1), mar = c(5, 10, 4, 2) + 0.1)
  barplot(y2, col = 'black', names.arg = depts_res, cex.names = 0.7, main = 'Sales by Department')
  barplot(y1, col = 'gray', add = TRUE)
  barplot(y3, col = 'white', add = TRUE)

  # Adding legend
  legend('topright', legend = c('With Defects', 'Without Defects'), fill = c('black', 'gray'))

  # Adding labels
  mtext('Sales', side = 1, line = 3, cex = 1.2)
  mtext('Departments', side = 2, line = 3, cex = 1.2)

  # Adjusting margins
  par(mar = c(5, 10, 4, 2) + 0.1)
}

```

The function of calculating the difference between average sales:
```{r}
different <- function(have_mark, no_mark) {
  dif <- numeric(length(depts_res))
  for (i in seq_along(depts_res)) {
    dif_add <- have_mark[i] - no_mark[i]
    dif[i] <- dif_add
  }
  return(dif)
}
```

The function of calculating the difference between average sales as a percentage:
```{r}
different_part <- function(have_mark, no_mark) {
  dif_part <- numeric(length(depts_res))
  for (i in seq_along(depts_res)) {
    dif_add <- (have_mark[i] - no_mark[i]) / abs(no_mark[i] / 100)
    dif_part[i] <- dif_add
  }
  return(dif_part)
}

```

Below is the Selection of records in which the values of all markdowns are equal to 0:




```{r}
# Assuming df is your data frame and depts_res is your departments vector
df_mark0 <- subset(merged_data, MarkDown1 == 0 & MarkDown2 == 0 & MarkDown3 == 0 & MarkDown4 == 0 & MarkDown5 == 0)
df_mark0_val <- filter_dept_sales(df_mark0)

# Graph output without markdowns
paint_graph(df_mark0_val, depts_res)

# Table output without markdowns
table_mark0 <- data.frame(
Department = depts_res, Sales = df_mark0_val)
table_mark0 <- table_mark0[order(table_mark0$Sales, decreasing = TRUE), ]

# Print the table
print(table_mark0)
```

The function of calculating average weekly sales and changes. Plotting if there is a markdown of a certain category:
```{r}
reserch_markdown <- function(number_MarkDown) {

  # Selection of records in which the values of markdowns of the nth type are greater than 0
  df_mark_have <- subset(merged_data, get(paste0("MarkDown", number_MarkDown)) > 0)


  # Determination of average weekly sales in the presence of markdown of the nth category
  df_markhave_val <- filter_dept_sales(df_mark_have)

  # Drawing a bar chart with sales figures before and after markdown
  paint_double_graph(depts_res, df_mark0_val, df_markhave_val)

  # Calculating changes
  dif_0have <- different(df_markhave_val, df_mark0_val)
  dif_part_0have <- different_part(df_markhave_val, df_mark0_val)

  
  depts <- lapply(depts_res, as.integer
               )
  depts_res <- unlist(depts)
  # Visual representation of changes
  # show_table_depts(depts_res, df_mark0_val, df_markhave_val, dif_0have, dif_part_0have, as.character(number_MarkDown))
  paint_graph(dif_0have, depts_res)
  paint_graph(dif_part_0have, depts_res)
}
```

Studying the impact of markdowns of the first category
```{r}
reserch_markdown(1)
```
# Figure 7. Holiday Reference for Corresponding Week Numbers

```{r, echo=FALSE}
# Create the Week column
merged_data$Week <- lubridate::week(merged_data$Date)
# Merge the holiday_table with the lookup table using IsHoliday as the k

# Convert the Date column to a Date format
merged_data$Date <- as.Date(merged_data$Date, format="%d/%m/%Y")

# Create the Week column
merged_data$Week <- lubridate::week(merged_data$Date)

# Create the Holiday_Ref column based on the specified holiday names
merged_data$Holiday_Ref <- ifelse(merged_data$Week == 6, "SuperBowl Week",
                           ifelse(merged_data$Week == 36, "LaborDay Week",
                           ifelse(merged_data$Week == 47|merged_data$Week == 48|
                                    merged_data$Week == 46, "ThanksGiving Week",
                           ifelse(merged_data$Week == 52, "Christmas Week", 
                                  "Non Holiday Week"))))

# Print the updated merged_data
head(merged_data)
```

# Figure 7. Holiday Reference with their respecive weeks
```{r, echo=FALSE}
# Filter merged_data to include only rows with holidays
holiday_week_ref <- merged_data %>%
  filter(Holiday_Ref != "Non Holiday Week") %>%
  select(Week, Holiday_Ref) %>%
  distinct()
head(holiday_week_ref)
```
Fig. 7. Holiday Reference for Corresponding Week Numbers

# Figure 8. Weekely sales vs Stores

```{r, echo=FALSE}
# Weekly_Sales vs. Store
ggplot(merged_data, aes(x = Store, y = Weekly_Sales, color = Holiday_Ref)) +
  geom_point() +
  labs(title = "Weekly Sales vs. Store",
       x = "Store",
       y = "Weekly Sales") +
scale_color_manual(values = c("Non Holiday Week" = "blue", "SuperBowl Week" = "yellow",
                        "LaborDay Week" = "green", "ThanksGiving Week" = "red",
                        "Christmas Week" = "purple"))
```

The analysis of weekly sales across various stores reveals a prominent surge in sales during Thanksgiving Week (Week 47), suggesting a notable correlation between this holiday period and heightened purchasing activity. Additionally, a general upward trend in holiday sales is observed across most stores, indicating increased customer spending during these periods. While Thanksgiving Week stands out as a high-sales period, store-specific variations in performance during other holidays highlight opportunities for tailored marketing and strategic promotions. The findings suggest the importance of understanding both overarching trends and individual store dynamics for effective sales strategies.

# Figure 9. Weekely sales vs Temperature
```{r, echo=FALSE}
# Weekly_Sales vs. Temperature
ggplot(merged_data)+
geom_point(aes(x = Temperature,y = Weekly_Sales,color = Holiday_Ref),size=0.7)+
scale_y_continuous(labels = scales::number_format(scale = 1e-3,suffix = "K"))+
scale_color_manual(values = c("Non Holiday Week" = "blue", "SuperBowl Week" = "yellow",
                        "LaborDay Week" = "green", "ThanksGiving Week" = "red",
                        "Christmas Week" = "purple")) +
  labs(title = "Weekly Sales vs. Temperature",
       x = "Temperature",
       y = "Weekly Sales") 
```

The examination of the weekly sales in relation to temperature patterns revealed noteworthy aspects into consumer behavior. The majority of sales happens within a temperature range of 19 to 79 degrees. This indicate a preference for moderate climatic conditions. Further, holidays such as Christmas, SuperBowl, and ThanksGiving showed distinct temperature-sales correlations which saw Christmas and ThanksGiving having increased sales in the range of 17 to 65 degrees, which aligns with a preference for cooler weather during these festivities. The Labor Day, conversely, shows elevated sales in the warmer range of 55 to 90 degrees. Moreover, ThanksGiving proves to be a pivotal driver of high sales as compared to sales in other holidays, emphasizing the significance of strategic planning and targeted marketing efforts during this holiday. This analysis gives the importance of considering holiday dynamics and temperature variations in crafting effective sales strategies.


```{r}
filtered_features_sales_merged <- features_sales_merged %>% group_by(Date) %>%
  summarise(Weekly_Sales = mean(Weekly_Sales), Temperature = mean(Temperature),
            Fuel_Price = mean(Fuel_Price), CPI = mean(CPI),
            Unemployment = mean(Unemployment), IsHoliday = sum(IsHoliday))

filtered_features_sales_merged$IsHoliday <- ifelse(
  filtered_features_sales_merged$IsHoliday!=0, TRUE, FALSE)
head(filtered_features_sales_merged)
```

## Analysing relation b/w Date vs Weekly_Sales, Temperature, Fuel_Price, CPI
##and Unemployment
```{r}
library(gridExtra)
x_axis <- ggplot(filtered_features_sales_merged, aes(x = Date))
plot1 <- x_axis +
  geom_line(aes(y = Weekly_Sales)) 
  #geom_point(data = subset(data, IsHoliday == TRUE), aes(y = Weekly_Sales), shape = 4, color = "red") +
  #labs(title = "Weekly Sales/sales on Holiday")

plot2 <- x_axis +
  geom_line(aes(y = Temperature)) +
  labs(title = "Temperature")

plot3 <- x_axis +
  geom_line(aes(y = Fuel_Price)) +
  labs(title = "Fuel_Price")

plot4 <- x_axis +
  geom_line(aes(y = CPI)) +
  labs(title = "CPI")

plot5 <- x_axis +
  geom_line(aes(y = Unemployment)) +
  labs(title = "Unemployment")

grid.arrange(plot1, plot2, plot3, plot4, plot5)
```


```{r}
#Chatgpt provided optimized code
library(tidyr)
plot_sales_features <- gather(filtered_features_sales_merged, "attribs",
                     "Value", -Date, -IsHoliday) 

ggplot(plot_sales_features, aes(Date, Value)) + geom_line(aes(color = Value),
                                                          linewidth = 1) + facet_grid(attribs~., scales = "free_y",switch = "y") +
  ylab(NULL) + 
  theme(strip.background = element_blank(), strip.placement = "outside", 
        strip.text.y.left = element_text(angle = 0), legend.position = "none")+
        scale_x_date(date_breaks = "5 months", date_labels = '%Y-%m')
```

The below are the results of the analysis:
1. The Weekly Sales are higher at the end of year, especially in the months of
    Nov-Dec, but over the year it is not increased.
2. The Weekly Sales are higher nearby holidays.
3. Fuel Price and CPI shown growth over the years.
4. Unemployment decreased year after the year.
5. Temperature is showing a random walk.
6. Weekly Sales trend of a year is similar to that of it's previous years, and
    hence, can be useful in predicting the sales trend.
7. It seems that weekly sales is not highly dependent on any of these factors
    as the sales show not much variability with any of these features.
    To confirm this we will use the correlation matrix.

```{r}
library(corrplot)
correlation_matrix <- cor(filtered_features_sales_merged[,
      c("Weekly_Sales", "Temperature", "Fuel_Price", "CPI", 
        "Unemployment")])
corrplot(correlation_matrix, method = "number")
```
The below are the results of the analysis:
1. Weekly sales does not show any high correlation with any other parameters.
2. CPI and Unemployment shows high negative correlation, this is expected as the
CPI increases, especially rapidly, it increases inflation and unemployment is an
effect of this.
3. CPI and Fuel_Price are highly positively correlated, this is also expected as
the CPI increase, indicates the increase in the price of fuel too.
4. Unemployment and Fuel_Price are negatively correlated, this can be considered
as effect of CPI on both unemployment and fuel_price.

```{r}
numeric_data <- select_if(merged_data, is.numeric)
correlation_matrix <- cor(numeric_data)
corrplot(correlation_matrix, method = "number")
```


##Analysing total yearly sales
```{r}
total_yearly_sales <- filtered_sales %>% group_by(Year) %>% 
                      summarise(Total_Sales = sum(Weekly_Sales))
head(total_yearly_sales)
```

```{r}
ggplot(total_yearly_sales,aes(x = Year, y = Total_Sales))+
  geom_bar(stat = "identity", fill = c("blue","orange","yellow"))+
  labs(title = "Total Sales vs Years")+
  scale_y_continuous(labels = scales::number_format(scale = 1e-6, suffix = "K"))
```

```{r}
plot1 <- ggplot(merged_data, mapping = aes(x = Type))+
  geom_bar(width = 0.3)+
  scale_y_continuous(labels = scales::number_format(scale = 1e-3, suffix = "K"))
plot2 <- ggplot(merged_data, mapping = aes(x = Type, y = Size))+
  geom_point()
plot3 <- ggplot(merged_data, mapping = aes(x = Type, y = Weekly_Sales))+
  geom_boxplot()+
  scale_y_continuous(limits = c(0, 6e4),
                  labels = scales::number_format(scale = 1e-3, suffix = "K"))
grid.arrange(plot1, plot2, plot3, nrow = 1)
```
The below are the results of the analysis:
1. Type A stores are the largest with highest sales, followed by B and then C.
2. We can notice a relationship between the size of retail stores and 
   the weekly sales, the bigger the size of the store, higher the weekly sales.


```{r}
# Figure 10 
# Weekly_Sales vs. Unemployment
ggplot(merged_data, aes(x = Unemployment, y = Weekly_Sales, color = Holiday_Ref)) +
  geom_point() +
  scale_y_continuous(labels = scales::number_format(scale = 1e-3,suffix = "K")) +
  labs(title = "Weekly Sales vs. Unemployment",
       x = "Unemployment",
       y = "Weekly_Sales") +
scale_color_manual(values = c("Non Holiday Week" = "blue", "SuperBowl Week" = "orange",
                        "LaborDay Week" = "green", "ThanksGiving Week" = "red",
                        "Christmas Week" = "purple"))
```
From the figure 10 we can observe unemployed people are least in visiting the store and on very rear cases people are visiting to the store on the major holidays like ThanksGiving.


#Figure 11

```{r}
ggplot(merged_data, aes(x = Size, y = Weekly_Sales, color = Holiday_Ref)) +
  geom_point() +
  scale_y_continuous(labels = scales::number_format(scale = 1e-3,suffix = "K")) +
  labs(title = "Weekly Sales vs. Store Size",
       x = "Store Size",
       y = "Weekly_Sales") +
scale_color_manual(values = c("Non Holiday Week" = "blue", "SuperBowl Week" = "orange",
                        "LaborDay Week" = "green", "ThanksGiving Week" = "red",
                        "Christmas Week" = "purple"))
```
# So, here the Store size also impacts the sales, which means that when the store size is lesser then the sales also will be lesser except on the important holidays like LaborDay, SuperBowl, ThanksGiving, and Christmas.


# Analysis of Markdown
```{r}
train_markdown <- merged_data[!is.na(merged_data$MarkDown2), ]

train_markdown <- aggregate(cbind(MarkDown1, MarkDown2, MarkDown3, MarkDown4, MarkDown5) ~ Date, data = train_markdown, mean)

ggplot(train_markdown, aes(x = Date)) +
  geom_line(aes(y = MarkDown1), color = "blue") +
  geom_line(aes(y = MarkDown2), color = "red") +
  geom_line(aes(y = MarkDown3), color = "green") +
  geom_line(aes(y = MarkDown4), color = "orange") +
  geom_line(aes(y = MarkDown5), color = "purple") +
  labs(title = "Timeline Markdown", y = "Markdown", x = "Date") +
  theme_minimal()  
```
```{r}
# Here in this plot we can observe that the mean sales of the year 2012 are higher and majorly markdown 3 is higher cross year 2012. As we can understand that the markdown 3 has impacted to higher sales at the end of the year 2011.


par(mfrow = c(2, 3), mar = c(4, 4, 2, 1))  # 2 rows, 3 columns

hist(train_markdown$MarkDown1, main = "MarkDown1", xlab = "Value", col = "lightgoldenrod", breaks = 6)
hist(train_markdown$MarkDown2, main = "MarkDown2", xlab = "Value", col = "lightgoldenrod", breaks = 6)
hist(train_markdown$MarkDown3, main = "MarkDown3", xlab = "Value", col = "lightgoldenrod", breaks = 6)
hist(train_markdown$MarkDown4, main = "MarkDown4", xlab = "Value", col = "lightgoldenrod", breaks = 6)
hist(train_markdown$MarkDown5, main = "MarkDown5", xlab = "Value", col = "lightgoldenrod", breaks = 6)

par(mfrow = c(1, 1), mar = c(4, 4, 2, 1))

```
Here, each graph depicts that how much of markdown has been done across different markdowns from 1 to 5. And frequency is the number of items that the markdown is being applied.

```{r}
train_markdown$Month <- format(train_markdown$Date, "%m")  # Extract month from Date

train_markdown_month <- aggregate(cbind(MarkDown1, MarkDown2, MarkDown3, MarkDown4, MarkDown5) ~ Month, data = train_markdown, mean)

barplot(t(as.matrix(train_markdown_month[, -1])), col = rainbow(5), legend.text = colnames(train_markdown_month)[-1], args.legend = list(x = "top"), main = "Stacked Monthwise Markdown", xlab = "Month", ylab = "Markdown")

```
```{r}

train_markdown_1 <- merged_data[!is.na(merged_data$MarkDown2), ]

train_markdown_type <- aggregate(cbind(MarkDown1, MarkDown2, MarkDown3, MarkDown4, MarkDown5) ~ Type, data = train_markdown_1, mean)

barplot(t(as.matrix(train_markdown_type[, -1])), col = rainbow(5), legend.text = colnames(train_markdown_type)[-1], args.legend = list(x = "topright"), main = "Stacked StoreType Wise", xlab = "Store Type", ylab = "Markdown")

```
Here, we observe which store type is using what markdown pricing majorly and through diagram we can see that there are 3 store types A, B, and C respectively. So, majorly Markdown 1 is being used for store types A and B. And least significantly Markdown 5 is being used for storetype C.
```{r}

unique_dept_values <- unique(merged_data$Dept)
num_unique_dept_values <- length(unique_dept_values)

print(num_unique_dept_values)

```


```{r}
library(dplyr)

data_Dept <- merged_data %>%
  group_by(Dept) %>%
  summarise(Weekly_Sales = sum(Weekly_Sales)) %>%
  ungroup() %>%
  mutate(Weekly_Sales = Weekly_Sales / 10000,
         Weekly_Sales = as.integer(Weekly_Sales)) %>%
  arrange(Weekly_Sales)
```


```{r}
data_Dept
```
```{r}

ggplot(data_Dept, aes(x = Dept, y = Weekly_Sales)) +
  geom_segment(aes(x = Dept, xend = Dept, y = 0, yend = Weekly_Sales), color = "skyblue") +
  geom_point(color = "blue", size = 3) +
  labs(title = "Departmentwise Sales", y = "Sales", x = "Department") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
It appears that departments 1-15 and 90-95, along with specific departments like
38, 40, and 72, have demonstrated higher weekly sales based on our analysis.
Understanding these patterns can be valuable for decision-making.

```{r}
 

sales_date_store <- aggregate(Weekly_Sales ~ Date + Store, data = merged_data, sum)
sales_date_store <- sales_date_store[order(sales_date_store$Date, sales_date_store$Store),]
sales_date_store$Weekly_Sales <- sales_date_store$Weekly_Sales / 10000
sales_date_store$Weekly_Sales <- as.integer(sales_date_store$Weekly_Sales)
data_table <- merge(features_data, sales_date_store, by = c("Date", "Store"), all.x = TRUE)
data_table <- merge(data_table, stores_data[c("Store", "Type")], by = "Store", all.x = TRUE)
head(data_table, 20)
```
```{r}
data_table$Year <- as.factor(format(data_table$Date, "%Y"))
ggplot(data_table, aes(x = Year, y = Weekly_Sales, fill = Type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Weekly Sales by Year and Store Type",
       x = "Year",
       y = "Weekly Sales") +
  theme_minimal()
```
In a detailed analysis, the data reveals that Store Type A and B have 
experienced a slight decline in sales on a yearly basis. Conversely, 
Store C has demonstrated a modest increase in sales over the same period. 
Further exploration of contributing factors could provide insights into these
trends.

```{r}
library(ggplot2)
data_table$Month <- as.factor(format(data_table$Date, "%m"))
ggplot(data_table, aes(x = Month, y = Weekly_Sales, fill = Type)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Monthly Sales by Store Type",
       x = "Month",
       y = "Weekly Sales") +
  scale_x_discrete(labels = month.abb) +  # Use abbreviated month names on x-axis
  theme_minimal()
```
Upon detailed analysis, it's observed that Type A and B stores experience a 
spike in sales during November and December, likely attributed to holiday 
seasons. Conversely, January shows a decrease in sales for these stores. In 
contrast, Type C store maintains consistency throughout every month, displaying 
minimal deviation in sales patterns. Understanding these monthly variations can 
aid in formulating targeted strategies for each store type.

```{r}
library(leaps)
regfit.full <- regsubsets(Weekly_Sales ~ ., merged_data, nvmax = 20)
reg.summary <- summary(regfit.full)
par(mfrow = c(1, 2))
plot(reg.summary$rss, xlab = "Number of Variables",
    ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables",
    ylab = "Adjusted RSq", type = "l")
par(mfrow = c(1,1))
```

```{r}
best_model_index <- which.max(reg.summary$adjr2)
best_model_features <- names(coef(regfit.full, id = best_model_index))
print(best_model_features)
```

```{r}
which.max(reg.summary$adjr2)
max(reg.summary$adjr2)
```

The best subset feature selection gives us all features instead of subset and
the low adjusted r-squared value improves the probability of the data
non-linearity.

#Training and test data split
```{r}
library(caret)
target <- merged_data$Weekly_Sales
splitIndex <- caret::createDataPartition(target, p = 0.8, list = FALSE)
train_data <- merged_data[splitIndex, ]
test_data <- merged_data[-splitIndex, ]
```

#Fitting Linear model to training data
```{r}
lin_reg <- lm(Weekly_Sales~., data = train_data)
summary(lin_reg)
```

```{r}
predictions<- predict(lin_reg, newdata = test_data)
RMSE <- sqrt(mean((test_data$Weekly_Sales - predictions)^2))
cat("\n The RMSE for the linear model:",RMSE)
```

```{r}
rsquared <- 1 - (sum((test_data$Weekly_Sales - predictions)^2)/
              sum((test_data$Weekly_Sales - mean(test_data$Weekly_Sales))^2))
cat("\n R-squared value for linear model:", rsquared)
```

The R-squared value is too low. This is indicating that the linear model is not
a reasonable fit as it is not capturing the underlying patterns in the data,
and it indicates the possibility of non-linearity in the data.

To confirm this, we check the Residual vs Fitted plot.

```{r}
plot(lin_reg)
```
From the above we can see that the residual vs fitted values plot is increasing
(almost like a cone shape), indicating heteroscedasticity, implying that the
variability of residuals is not constant across the fitted values' levels.

As the data has non-linearity and choosing non-linear models is a good
option. Hence, we chose RandomForest to capture the non-linear patterns in the
data. The RandomForest's robustness to overfitting and implicit feature
engineering makes it a good option for non-linear datasets.

```{r}
library(randomForest)

set.seed(123)

```



```{r}
results <- data.frame(mtry = numeric(), rmse = numeric())
ctrl <- trainControl(method = "cv", number = 5)
param_grid <- expand.grid(
  mtry = c(2,4,6))

for (m in param_grid$mtry) {
  rf_model <- train(
    Weekly_Sales ~ .,
    data = train_data,
    method = "rf",
    metric = "RMSE",
    trControl = ctrl,
    tuneGrid = data.frame(mtry = m),
    ntree = 10
  )
  
  predictions <- predict(rf_model, newdata = test_data)
  rmse <- sqrt(mean((test_data$Weekly_Sales - predictions)^2))
  results <- rbind(results, data.frame(mtry = m, rmse = rmse))
}

ggplot(results, aes(x = mtry, y = rmse)) +
  geom_line() +
  geom_point() +
  labs(title = "RMSE vs mtry",
       x = "mtry",
       y = "Root Mean Squared Error (RMSE)")
```

#Fitting Randomforest model
```{r}
rf_model <- randomForest(x = train_data[,-c(4,5,8,10)],
                         y = train_data$Weekly_Sales,
                         ntree = 10, mtry = 6)
```

#Feature importance from Randomforest
Let us extract important features from Randomforest model
```{r}
feature_importance <- importance(rf_model)
print("Feature Importance:")
print(feature_importance)
varImpPlot(rf_model)
```
From the above, we see that Department, Size and Store are most important 
features in predicting weekly sales.This inturn means:

1. Store-related factors such as size, specific store identity, and department 
    contribute significantly to weekly sales predictions.
2. CPI and Unemployment have a noteworthy impact on sales.
3. Promotional events have less effect on the sales when compared to external
   factors.

#Model evaluation
```{r}
predictions <- predict(rf_model, newdata = test_data[,-c(4,5,8,10)])
mae <- mean(abs(test_data$Weekly_Sales - predictions))
print(paste("Mean Absolute Error (MAE):", mae))

mse <- mean((test_data$Weekly_Sales - predictions)^2)
print(paste("Root Mean Squared Error (MSE):", sqrt(mse)))

rsquared_rf <- 1 - (sum((test_data$Weekly_Sales - predictions)^2) / 
                sum((test_data$Weekly_Sales - mean(test_data$Weekly_Sales))^2))
print(paste("R-squared:", rsquared_rf))
```


```{r}
metrics_df <- data.frame(
  Model = c("Linear Model", "Random Forest"),
  RMSE = c(RMSE, sqrt(mse)),
  R2 = c(rsquared, rsquared_rf)
)

ggplot(metrics_df, aes(x = Model, y = RMSE, fill = Model)) +
  geom_point(stat = "identity") +
  labs(title = "Root Mean Squared Error (RMSE) Comparison")

ggplot(metrics_df, aes(x = Model, y = R2, fill = Model)) +
  geom_point(stat = "identity") +
  labs(title = "R-squared (R2) Comparison")
```

The R-squared value of the Randomforest is quite high when compared to the
linear model. This explains that the Randomforest can efficiently explain the 
variability of data, than the linear model.



